{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e92a4d-ad5e-40aa-9831-070f873cd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b017914-d0c8-4ad0-baec-5dd708639c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "df = pd.read_csv(\"archive/blogtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57223c65-4dc5-4b86-999c-2274bad546e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove non-alphanumeric characters and extra spaces\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02ae42-ea91-4dd5-8a66-55cf7a08c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a3836-ebfd-470e-ad44-ca4042648c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Encode the gender column\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender_encoded'] = label_encoder.fit_transform(df['gender'])  # Male: 0, Female: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78101e-8a4d-46c9-a26b-fbe7b38e566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Train-test split\n",
    "X = df['clean_text']  # Features (text data)\n",
    "y = df['gender_encoded']  # Target (encoded gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221eaa72-e9d6-4f1d-8069-81d90e1ce1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f02338-3197-4b85-95cd-65458ce8b9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\STUDY\\MS\\DNSC Natural Language Processing\\project\\model2.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#%%\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m device \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d872126-4333-41ee-b5ab-dc9d97da84e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pretrained DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2923c19-4935-47cd-b3d9-dfc96f283de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 545027/545027 [1:27:36<00:00, 103.69it/s] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\STUDY\\MS\\DNSC Natural Language Processing\\project\\model2.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(embeddings, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Return as a single tensor\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Generate embeddings for training and testing datasets\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m X_train_embeddings \u001b[39m=\u001b[39m generate_embeddings(X_train\u001b[39m.\u001b[39mtolist(), tokenizer, model)\n\u001b[0;32m     27\u001b[0m X_test_embeddings \u001b[39m=\u001b[39m generate_embeddings(X_test\u001b[39m.\u001b[39mtolist(), tokenizer, model)\n",
      "File \u001b[1;32md:\\STUDY\\MS\\DNSC Natural Language Processing\\project\\model2.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m         sentence_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(token_embeddings, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Mean pooling over tokens\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(sentence_embedding\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m---> 23\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(embeddings, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def generate_embeddings(texts, tokenizer, model, max_length=128):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating Embeddings\"):\n",
    "        # Tokenize text\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens['input_ids'].to(device)\n",
    "        attention_mask = tokens['attention_mask'].to(device)\n",
    "\n",
    "        # Pass through model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            token_embeddings = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "            sentence_embedding = torch.mean(token_embeddings, dim=1)  # Mean pooling over tokens\n",
    "            embeddings.append(sentence_embedding.cpu().numpy())\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)  # Return as a single tensor\n",
    "\n",
    "# Generate embeddings for training and testing datasets\n",
    "X_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\n",
    "X_test_embeddings = generate_embeddings(X_test.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9108c-2e84-4834-b726-55b6e328ce73",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Flatten embeddings for classifier input\n",
    "X_train_embeddings = X_train_embeddings.squeeze()\n",
    "X_test_embeddings = X_test_embeddings.squeeze()\n",
    "\n",
    "# Train Logistic Regression on the extracted embeddings\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9f74c-1486-45b5-aca0-c642158f80ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\STUDY\\MS\\DNSC Natural Language Processing\\project\\model2.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Flatten embeddings for classifier input\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train_embeddings \u001b[39m=\u001b[39m X_train_embeddings\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m      4\u001b[0m X_test_embeddings \u001b[39m=\u001b[39m X_test_embeddings\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m      6\u001b[0m \u001b[39m# Train Logistic Regression on the extracted embeddings\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Flatten embeddings for classifier input\n",
    "X_train_embeddings = X_train_embeddings.squeeze()\n",
    "X_test_embeddings = X_test_embeddings.squeeze()\n",
    "\n",
    "# Train Logistic Regression on the extracted embeddings\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test_embeddings)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
